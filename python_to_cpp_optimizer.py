# -*- coding: utf-8 -*-
"""python_to_cpp_optimizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FyJyRqMhISa9NFp8UoOj5MojmBnMZp9U
"""

# imports

import os
import gradio as gr
from IPython.display import Markdown, display, update_display
from openai import OpenAI
from huggingface_hub import login
from google.colab import userdata
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig

hf_token = userdata.get('HUGGINGFACE_API_KEY')
login(hf_token, add_to_git_credential=True)

openai_api_key = userdata.get('OPENAI_API_KEY')
openai = OpenAI(api_key=openai_api_key)

system_message = "You are an assistant that reimplements Python code in high performance C++ for an M1 Mac. "
system_message += "Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. "
system_message += "The C++ response needs to produce an identical output in the fastest possible time. Keep implementations of random number generators identical so that results match exactly."

def user_prompt_for(python):
    user_prompt = "Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. "
    user_prompt += "Respond only with C++ code; do not explain your work other than a few comments. "
    user_prompt += "Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\n\n"
    user_prompt += python
    return user_prompt

def messages_for(python):
    return [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_prompt_for(python)}
    ]

# write to a file called optimized.cpp

def write_output(cpp):
    code = cpp.replace("```cpp","").replace("```","")
    with open("optimized.cpp", "w") as f:
        f.write(code)

def optimize_claude(python):
    result = claude.messages.stream(
        model=CLAUDE_MODEL,
        max_tokens=2000,
        system=system_message,
        messages=[{"role": "user", "content": user_prompt_for(python)}],
    )
    reply = ""
    with result as stream:
        for text in stream.text_stream:
            reply += text
            print(text, end="", flush=True)
    write_output(reply)

python_hard = """# Be careful to support large number sizes

def lcg(seed, a=1664525, c=1013904223, m=2**32):
    value = seed
    while True:
        value = (a * value + c) % m
        yield value

def max_subarray_sum(n, seed, min_val, max_val):
    lcg_gen = lcg(seed)
    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]
    max_sum = float('-inf')
    for i in range(n):
        current_sum = 0
        for j in range(i, n):
            current_sum += random_numbers[j]
            if current_sum > max_sum:
                max_sum = current_sum
    return max_sum

def total_max_subarray_sum(n, initial_seed, min_val, max_val):
    total_sum = 0
    lcg_gen = lcg(initial_seed)
    for _ in range(20):
        seed = next(lcg_gen)
        total_sum += max_subarray_sum(n, seed, min_val, max_val)
    return total_sum

# Parameters
n = 10000         # Number of random numbers
initial_seed = 42 # Initial seed for the LCG
min_val = -10     # Minimum value of random numbers
max_val = 10      # Maximum value of random numbers

# Timing the function
import time
start_time = time.time()
result = total_max_subarray_sum(n, initial_seed, min_val, max_val)
end_time = time.time()

print("Total Maximum Subarray Sum (20 runs):", result)
print("Execution Time: {:.6f} seconds".format(end_time - start_time))
"""

exec(python_hard)

model_name = "gpt-4o"

def optimize_gpt(python, model_name):
    stream = openai.chat.completions.create(model=model_name, messages=messages_for(python), stream=True)
    reply = ""
    for chunk in stream:
        fragment = chunk.choices[0].delta.content or ""
        reply += fragment
        print(fragment, end='', flush=True)
    write_output(reply)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile optimized.cpp
# #include <iostream>
# #include <vector>
# #include <limits>
# #include <cstdint>
# #include <chrono>
# 
# // Linear Congruential Generator function
# uint32_t lcg(uint32_t &seed, uint32_t a = 1664525, uint32_t c = 1013904223) {
#     seed = (static_cast<uint64_t>(a) * seed + c) % 0x100000000; // Explicit modulo 2^32
#     return seed;
# }
# 
# // Function to compute maximum subarray sum
# int max_subarray_sum(int n, uint32_t seed, int min_val, int max_val) {
#     std::vector<int> random_numbers(n);
#     uint32_t current_seed = seed; // Use a local copy of the seed
#     for (int i = 0; i < n; ++i) {
#         random_numbers[i] = lcg(current_seed) % (max_val - min_val + 1) + min_val;
#     }
#     int max_sum = std::numeric_limits<int>::min();
#     for (int i = 0; i < n; ++i) {
#         int current_sum = 0;
#         for (int j = i; j < n; ++j) {
#             current_sum += random_numbers[j];
#             if (current_sum > max_sum) {
#                 max_sum = current_sum;
#             }
#         }
#     }
#     return max_sum;
# }
# 
# // Function to compute total maximum subarray sum over 20 runs
# int total_max_subarray_sum(int n, uint32_t initial_seed, int min_val, int max_val) {
#     int total_sum = 0;
#     uint32_t current_seed = initial_seed; // Use a local copy of the seed
#     for (int i = 0; i < 20; ++i) {
#         uint32_t seed = lcg(current_seed);
#         total_sum += max_subarray_sum(n, seed, min_val, max_val);
#     }
#     return total_sum;
# }
# 
# int main() {
#     int n = 10000;          // Number of random numbers
#     uint32_t initial_seed = 42; // Initial seed for the LCG
#     int min_val = -10;      // Minimum value of random numbers
#     int max_val = 10;       // Maximum value of random numbers
# 
#     // Timing the function
#     auto start_time = std::chrono::high_resolution_clock::now();
#     int result = total_max_subarray_sum(n, initial_seed, min_val, max_val);
#     auto end_time = std::chrono::high_resolution_clock::now();
# 
#     std::chrono::duration<double> execution_time = end_time - start_time;
# 
#     std::cout << "Total Maximum Subarray Sum (20 runs): " << result << std::endl;
#     std::cout << "Execution Time: " << execution_time.count() << " seconds" << std::endl;
# 
#     return 0;
# }

!clang++ -O3 -std=c++17 -march=native -o optimized optimized.cpp
!./optimized

def stream_gpt(python):
    stream = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for(python), stream=True)
    reply = ""
    for chunk in stream:
        fragment = chunk.choices[0].delta.content or ""
        reply += fragment
        yield reply.replace('```cpp\n','').replace('```','')

def stream_claude(python):
    result = claude.messages.stream(
        model=CLAUDE_MODEL,
        max_tokens=2000,
        system=system_message,
        messages=[{"role": "user", "content": user_prompt_for(python)}],
    )
    reply = ""
    with result as stream:
        for text in stream.text_stream:
            reply += text
            yield reply.replace('```cpp\n','').replace('```','')

def optimize(python, model):
    if model=="GPT":
        result = stream_gpt(python)
    elif model=="Claude":
        result = stream_claude(python)
    else:
        raise ValueError("Unknown model")
    for stream_so_far in result:
        yield stream_so_far

with gr.Blocks() as ui:
    with gr.Row():
        python = gr.Textbox(label="Python code:", lines=10, value=python_hard)
        cpp = gr.Textbox(label="C++ code:", lines=10)
    with gr.Row():
        model = gr.Dropdown(["GPT", "Claude"], label="Select model", value="GPT")
        convert = gr.Button("Convert code")

    convert.click(optimize, inputs=[python, model], outputs=[cpp])

ui.launch(inbrowser=True)

def execute_python(code):
    try:
        output = io.StringIO()
        sys.stdout = output
        exec(code)
    finally:
        sys.stdout = sys.__stdout__
    return output.getvalue()

def execute_cpp(code):
    write_output(code)
    compiler_cmd = ["clang++", "-O3", "-std=c++17", "-march=armv8.3-a", "-o", "optimized", "optimized.cpp"]
    try:
        compile_result = subprocess.run(compiler_cmd, check=True, text=True, capture_output=True)
        run_cmd = ["./optimized"]
        run_result = subprocess.run(run_cmd, check=True, text=True, capture_output=True)
        return run_result.stdout
    except subprocess.CalledProcessError as e:
        return f"An error occurred:\n{e.stderr}"

css = """
.python {background-color: #306998;}
.cpp {background-color: #050;}
"""

with gr.Blocks(css=css) as ui:
    gr.Markdown("## Convert code from Python to C++")
    with gr.Row():
        python = gr.Textbox(label="Python code:", value=python_hard, lines=10)
        cpp = gr.Textbox(label="C++ code:", lines=10)
    with gr.Row():
        model = gr.Dropdown(["GPT", "Claude"], label="Select model", value="GPT")
    with gr.Row():
        convert = gr.Button("Convert code")
    with gr.Row():
        python_run = gr.Button("Run Python")
        cpp_run = gr.Button("Run C++")
    with gr.Row():
        python_out = gr.TextArea(label="Python result:", elem_classes=["python"])
        cpp_out = gr.TextArea(label="C++ result:", elem_classes=["cpp"])

    convert.click(optimize, inputs=[python, model], outputs=[cpp])
    python_run.click(execute_python, inputs=[python], outputs=[python_out])
    cpp_run.click(execute_cpp, inputs=[cpp], outputs=[cpp_out])

ui.launch(inbrowser=True)

